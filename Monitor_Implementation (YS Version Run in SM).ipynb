{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zK90-V49ggfZ"
   },
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5OFz_uEbmi0"
   },
   "source": [
    "Imports & AWS Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T05:13:37.800296Z",
     "iopub.status.busy": "2025-10-21T05:13:37.799885Z",
     "iopub.status.idle": "2025-10-21T05:13:38.150920Z",
     "shell.execute_reply": "2025-10-21T05:13:38.150217Z",
     "shell.execute_reply.started": "2025-10-21T05:13:37.800270Z"
    },
    "id": "SXoNZKbIKreQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Bucket: sagemaker-us-east-1-767397858887\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.model_monitor import DefaultModelMonitor, ModelQualityMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Initialize AWS clients\n",
    "sm_sess = sagemaker.Session()\n",
    "BUCKET = sm_sess.default_bucket()\n",
    "ROLE = sagemaker.get_execution_role()\n",
    "REGION = sm_sess.boto_region_name\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "cw = boto3.client(\"cloudwatch\")\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"Bucket: {BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NQ_O7bncSLy"
   },
   "source": [
    "Define S3 Paths & Monitoring Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:14:09.464056Z",
     "iopub.status.busy": "2025-10-21T02:14:09.463818Z",
     "iopub.status.idle": "2025-10-21T02:14:09.470287Z",
     "shell.execute_reply": "2025-10-21T02:14:09.469555Z",
     "shell.execute_reply.started": "2025-10-21T02:14:09.464033Z"
    },
    "id": "Z-sib0TjcShr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name: student-anxiety-ml-xgb-batch-1760782623\n",
      "Reports: s3://sagemaker-us-east-1-767397858887/sm-batch-monitoring/reports\n"
     ]
    }
   ],
   "source": [
    "# Monitoring Setup & Configuration (bucket, job, and local dirs)\n",
    "DATA_PREFIX = \"student-anxiety-ml\"\n",
    "JOB_NAME = os.environ.get(\"BATCH_JOB_NAME\", \"student-anxiety-ml-xgb-batch-1760782623\")\n",
    "LOCAL_DIR = \"local_artifacts\"\n",
    "os.makedirs(LOCAL_DIR, exist_ok=True)\n",
    "\n",
    "# S3 URIs\n",
    "TRAIN_XGB   = f\"s3://{BUCKET}/{DATA_PREFIX}/xgb/train/train.csv\"          # label-first, no header\n",
    "BATCH_IN    = f\"s3://{BUCKET}/{DATA_PREFIX}/batch-inference/input/\"       # features-only, no header\n",
    "BATCH_OUT   = f\"s3://{BUCKET}/{DATA_PREFIX}/batch-predictions/xgb/\"       # .out softprob files\n",
    "PROD_XGB    = f\"s3://{BUCKET}/{DATA_PREFIX}/xgb/prod/prod.csv\"            # label-first, for GT\n",
    "\n",
    "PREFIX      = \"sm-batch-monitoring\"\n",
    "BASE_S3     = f\"s3://{BUCKET}/{PREFIX}\"\n",
    "REPORTS     = f\"{BASE_S3}/reports\"\n",
    "BASELINES   = f\"{BASE_S3}/baseline\"\n",
    "\n",
    "print(f\"Job Name: {JOB_NAME}\")\n",
    "print(f\"Reports: {REPORTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gC8oMsXQglbE"
   },
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oyj82S6hKs5u"
   },
   "source": [
    "Define S3 Helper Functions for CSV Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:14:09.471417Z",
     "iopub.status.busy": "2025-10-21T02:14:09.470981Z",
     "iopub.status.idle": "2025-10-21T02:14:09.509851Z",
     "shell.execute_reply": "2025-10-21T02:14:09.508725Z",
     "shell.execute_reply.started": "2025-10-21T02:14:09.471386Z"
    },
    "id": "kmWlCgCoKmY6"
   },
   "outputs": [],
   "source": [
    "def s3_split(uri):\n",
    "    \"\"\"Split S3 URI into bucket & key\"\"\"\n",
    "    assert uri.startswith(\"s3://\")\n",
    "    rest = uri[5:]\n",
    "    bucket, key = rest.split(\"/\", 1)\n",
    "    return bucket, key\n",
    "\n",
    "def download_process_upload_csv(source_uri, dest_key, process_fn):\n",
    "    \"\"\"\n",
    "    Downloads CSV from S3, transforms it, uploads result.\n",
    "    Needed for both baseline prep & GT upload\n",
    "    \"\"\"\n",
    "    # Download from S3 to memory\n",
    "    src_bucket, src_key = s3_split(source_uri)\n",
    "    body = s3.get_object(Bucket=src_bucket, Key=src_key)[\"Body\"].read()\n",
    "\n",
    "    # Process DF\n",
    "    df = pd.read_csv(io.BytesIO(body), header=None)\n",
    "    result = process_fn(df)\n",
    "\n",
    "    # Upload back to S3\n",
    "    csv_bytes = result.to_csv(header=False, index=False).encode()\n",
    "    s3.put_object(Bucket=BUCKET, Key=dest_key, Body=csv_bytes)\n",
    "\n",
    "    return f\"s3://{BUCKET}/{dest_key}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oi5lbs4wee7j"
   },
   "source": [
    "Define Data Prep Functions (Baseline, Predictions, Ground Truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T07:54:00.280526Z",
     "iopub.status.busy": "2025-10-21T07:54:00.280248Z",
     "iopub.status.idle": "2025-10-21T07:54:00.289275Z",
     "shell.execute_reply": "2025-10-21T07:54:00.288340Z",
     "shell.execute_reply.started": "2025-10-21T07:54:00.280505Z"
    },
    "id": "tr4FZyaZedbe"
   },
   "outputs": [],
   "source": [
    "def ensure_features_only_baseline():\n",
    "    \"\"\"\n",
    "    Creates baseline for data quality monitoring.\n",
    "    Needed bc SageMaker's DefaultModelMonitor expects baseline to match batch input format (features only, no labels).\n",
    "    Our training data has labels so we strip col 0.\n",
    "\n",
    "    Returns S3 path to the features-only baseline.\n",
    "    \"\"\"\n",
    "    dst_key = f\"{DATA_PREFIX}/xgb/train_features_only/train_features_only.csv\"\n",
    "\n",
    "    # Download from S3, remove label column (first col), upload result\n",
    "    def drop_label_column(df):\n",
    "        return df.iloc[:, 1:]\n",
    "\n",
    "    download_process_upload_csv(TRAIN_XGB, dst_key, drop_label_column)\n",
    "    return f\"s3://{BUCKET}/{DATA_PREFIX}/xgb/train_features_only/\"\n",
    "\n",
    "def write_pred_labels_from_softprob():\n",
    "    \"\"\"\n",
    "    TL;DR Converts batch transform output to predicted class labels.\n",
    "\n",
    "    Batch transform outputs softmax probs in .out files (one per shard).\n",
    "    This reads them all, takes argmax to get predicted class,\n",
    "    concatenates into single CSV for model quality monitor.\n",
    "\n",
    "    NOTE: Make sure batch job finished before running this to avoid\n",
    "    RuntimeError about no .out files.\n",
    "    \"\"\"\n",
    "    batch_bucket, batch_prefix = s3_split(BATCH_OUT)\n",
    "    keys = []\n",
    "    for page in s3.get_paginator(\"list_objects_v2\").paginate(Bucket=batch_bucket, Prefix=batch_prefix):\n",
    "        keys += [o[\"Key\"] for o in page.get(\"Contents\",[]) if o[\"Key\"].endswith(\".out\")]\n",
    "\n",
    "    if not keys:\n",
    "        raise RuntimeError(\"No .out files found under BATCH_OUT.\")\n",
    "    keys.sort()\n",
    "\n",
    "    parts = []\n",
    "    for k in keys:\n",
    "        body = s3.get_object(Bucket=batch_bucket, Key=k)[\"Body\"].read().decode('utf-8')\n",
    "        \n",
    "        # Parse each line as JSON and convert to numpy array\n",
    "        probs_list = []\n",
    "        for line in body.strip().split('\\n'):\n",
    "            if line.strip():\n",
    "                probs_list.append(json.loads(line))\n",
    "        \n",
    "        # Convert to numpy array & get argmax\n",
    "        probs_array = pd.DataFrame(probs_list).values\n",
    "        pred_classes = probs_array.argmax(axis=1)\n",
    "        parts.append(pd.DataFrame(pred_classes))\n",
    "\n",
    "    preds = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "    out_key = f\"{DATA_PREFIX}/batch-predictions/xgb_post/preds_labels.csv\"\n",
    "    s3.put_object(Bucket=BUCKET, Key=out_key, Body=preds.to_csv(header=False, index=False).encode())\n",
    "\n",
    "    return f\"s3://{BUCKET}/{out_key}\"\n",
    "\n",
    "def upload_ground_truth_from_prod():\n",
    "    \"\"\"\n",
    "    tl;dr Pulls ground truth labels (col 0) from prod.csv for model quality eval.\n",
    "    Our prod.csv has the same format as training data (label in column 0).\n",
    "    We pull just that column and upload it for model quality evaluation.\n",
    "    \"\"\"\n",
    "    gt_key = f\"{DATA_PREFIX}/ground-truth/ground_truth.csv\"\n",
    "\n",
    "    # Download prod data, extract ground truth label (col 0), upload result\n",
    "    def extract_label_column(df):\n",
    "        return df[[0]]\n",
    "\n",
    "    return download_process_upload_csv(PROD_XGB, gt_key, extract_label_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdL8AZ9QenbE"
   },
   "source": [
    "Define CloudWatch Metrics & Dashboard Creation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:14:09.524294Z",
     "iopub.status.busy": "2025-10-21T02:14:09.524005Z",
     "iopub.status.idle": "2025-10-21T02:14:09.533525Z",
     "shell.execute_reply": "2025-10-21T02:14:09.532665Z",
     "shell.execute_reply.started": "2025-10-21T02:14:09.524268Z"
    },
    "id": "7VKaBuuxK2Ql"
   },
   "outputs": [],
   "source": [
    "def monitor_infra(job_name):\n",
    "    \"\"\"Pushes job duration & success metrics to CloudWatch.\"\"\"\n",
    "\n",
    "    # Get job details\n",
    "    response = sm.describe_transform_job(TransformJobName=job_name)\n",
    "    status = response[\"TransformJobStatus\"]\n",
    "\n",
    "    # If job hasn't completed, record failure\n",
    "    if \"TransformEndTime\" not in response:\n",
    "        metric_data = [{\n",
    "            \"MetricName\": \"JobSuccess\",\n",
    "            \"Dimensions\": [{\"Name\": \"TransformJobName\", \"Value\": job_name}],\n",
    "            \"Value\": 0,\n",
    "            \"Unit\": \"Count\"\n",
    "        }]\n",
    "        cw.put_metric_data(\n",
    "            Namespace  = \"SageMaker/Batch/Infrastructure\",\n",
    "            MetricData = metric_data\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # Calculate job duration\n",
    "    start_time = response[\"TransformStartTime\"]\n",
    "    end_time = response[\"TransformEndTime\"]\n",
    "    duration_seconds = (end_time - start_time).total_seconds()\n",
    "\n",
    "    # Determine success status\n",
    "    success_value = 1 if status == \"Completed\" else 0\n",
    "\n",
    "    # Publish both metrics\n",
    "    metric_data = [\n",
    "        {\n",
    "            \"MetricName\": \"JobDuration\",\n",
    "            \"Dimensions\": [{\"Name\": \"TransformJobName\", \"Value\": job_name}],\n",
    "            \"Value\": duration_seconds,\n",
    "            \"Unit\": \"Seconds\"\n",
    "        },\n",
    "        {\n",
    "            \"MetricName\": \"JobSuccess\",\n",
    "            \"Dimensions\": [{\"Name\": \"TransformJobName\", \"Value\": job_name}],\n",
    "            \"Value\": success_value,\n",
    "            \"Unit\": \"Count\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    cw.put_metric_data(\n",
    "        Namespace  = \"SageMaker/Batch/Infrastructure\",\n",
    "        MetricData = metric_data\n",
    "    )\n",
    "\n",
    "\n",
    "def create_dashboard(name=\"SageMaker-Batch-Monitoring-Dashboard\"):\n",
    "    \"\"\"\n",
    "    Creates CloudWatch dashboard for monitoring batch inference pipeline.\n",
    "\n",
    "    Shows job duration, success rate, and links to S3 monitoring reports.\n",
    "    We can view it in the CloudWatch console (after running this).\n",
    "    \"\"\"\n",
    "    # Dashboard configuration\n",
    "    dashboard_body = {\n",
    "        \"widgets\": [\n",
    "            # Job Duration Widget\n",
    "            {\n",
    "                \"type\": \"metric\",\n",
    "                \"width\": 12,\n",
    "                \"height\": 6,\n",
    "                \"properties\": {\n",
    "                    \"metrics\": [[\n",
    "                        \"SageMaker/Batch/Infrastructure\",\n",
    "                        \"JobDuration\",\n",
    "                        \"TransformJobName\",\n",
    "                        JOB_NAME\n",
    "                    ]],\n",
    "                    \"period\": 300,\n",
    "                    \"stat\": \"Average\",\n",
    "                    \"region\": REGION,\n",
    "                    \"title\": \"Batch Job Duration (s)\"\n",
    "                }\n",
    "            },\n",
    "            # Job Success Widget\n",
    "            {\n",
    "                \"type\": \"metric\",\n",
    "                \"width\": 12,\n",
    "                \"height\": 6,\n",
    "                \"properties\": {\n",
    "                    \"metrics\": [[\n",
    "                        \"SageMaker/Batch/Infrastructure\",\n",
    "                        \"JobSuccess\",\n",
    "                        \"TransformJobName\",\n",
    "                        JOB_NAME\n",
    "                    ]],\n",
    "                    \"period\": 300,\n",
    "                    \"stat\": \"Sum\",\n",
    "                    \"region\": REGION,\n",
    "                    \"title\": \"Batch Job Success\"\n",
    "                }\n",
    "            },\n",
    "            # Reports Summary Widget\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"width\": 24,\n",
    "                \"height\": 4,\n",
    "                \"properties\": {\n",
    "                    \"markdown\": (\n",
    "                        f\"### Reports\\n\"\n",
    "                        f\"- Data Quality: `{REPORTS}/data_quality_analysis`\\n\"\n",
    "                        f\"- Model Quality: `{REPORTS}/quality_analysis`\"\n",
    "                    )\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    cw.put_dashboard(\n",
    "        DashboardName = name,\n",
    "        DashboardBody = json.dumps(dashboard_body)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4k9ubTR6LMLb"
   },
   "source": [
    "# Monitoring Pipeline Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5s33QXRfAGe"
   },
   "source": [
    "Infrastructure Monitoring - Publish Job Metrics to CloudWatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T02:14:09.535689Z",
     "iopub.status.busy": "2025-10-21T02:14:09.535280Z",
     "iopub.status.idle": "2025-10-21T02:14:09.666399Z",
     "shell.execute_reply": "2025-10-21T02:14:09.665496Z",
     "shell.execute_reply.started": "2025-10-21T02:14:09.535660Z"
    },
    "id": "ZmfsMR3lfDu9"
   },
   "outputs": [],
   "source": [
    "# Infrastructure Monitoring - Track job performance\n",
    "monitor_infra(JOB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Cuc8KPhfHht"
   },
   "source": [
    " Data Quality Monitoring - Create Baseline & Detect Data Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T07:56:14.860253Z",
     "iopub.status.busy": "2025-10-21T07:56:14.859994Z",
     "iopub.status.idle": "2025-10-21T08:07:41.789459Z",
     "shell.execute_reply": "2025-10-21T08:07:41.788653Z",
     "shell.execute_reply.started": "2025-10-21T07:56:14.860233Z"
    },
    "id": "YTto9wAAfzx5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2025-10-21-07-56-15-318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................................................!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2025-10-21-08-01-18-149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................................................................!"
     ]
    }
   ],
   "source": [
    "# Data Quality Monitoring - Detect drift vs training data (baseline + run)\n",
    "\n",
    "data_monitor = DefaultModelMonitor(\n",
    "    role=ROLE,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    sagemaker_session=sm_sess\n",
    ")\n",
    "\n",
    "# Generate baseline statistics\n",
    "data_monitor.suggest_baseline(\n",
    "    baseline_dataset=ensure_features_only_baseline(),\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    "    output_s3_uri=f\"{BASELINES}/data\",\n",
    "    wait=True,\n",
    "    logs=False\n",
    ")\n",
    "\n",
    "# Generate statistics for batch inference data\n",
    "data_monitor.suggest_baseline(\n",
    "    baseline_dataset=BATCH_IN,\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    "    output_s3_uri=f\"{REPORTS}/data_quality_analysis\",\n",
    "    wait=True,\n",
    "    logs=False\n",
    ")\n",
    "\n",
    "# Download & compare statistics\n",
    "baseline_stats_key = f\"{PREFIX}/baseline/data/statistics.json\"\n",
    "batch_stats_key = f\"{PREFIX}/reports/data_quality_analysis/statistics.json\"\n",
    "\n",
    "baseline_stats = json.loads(s3.get_object(Bucket=BUCKET, Key=baseline_stats_key)[\"Body\"].read())\n",
    "batch_stats = json.loads(s3.get_object(Bucket=BUCKET, Key=batch_stats_key)[\"Body\"].read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T08:35:18.087246Z",
     "iopub.status.busy": "2025-10-21T08:35:18.086876Z",
     "iopub.status.idle": "2025-10-21T08:35:18.094529Z",
     "shell.execute_reply": "2025-10-21T08:35:18.093673Z",
     "shell.execute_reply.started": "2025-10-21T08:35:18.087206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Quality Summary:\n",
      "  _c0: OK (baseline: 15.78, batch: 15.78, drift: 0.0%)\n",
      "  _c1: DRIFT (baseline: -0.01, batch: -0.01, drift: 46.4%)\n",
      "  _c2: OK (baseline: 0.48, batch: 0.49, drift: 1.3%)\n",
      "  _c3: DRIFT (baseline: 0.17, batch: 0.20, drift: 15.6%)\n",
      "  _c4: DRIFT (baseline: 0.25, batch: 0.27, drift: 11.5%)\n",
      "  _c5: DRIFT (baseline: 0.25, batch: 0.29, drift: 13.9%)\n",
      "  _c6: OK (baseline: 0.45, batch: 0.49, drift: 7.7%)\n",
      "  _c7: OK (baseline: -0.05, batch: -0.05, drift: 0.2%)\n",
      "  _c8: OK (baseline: 10.14, batch: 10.10, drift: 0.3%)\n",
      "  _c9: DRIFT (baseline: 0.11, batch: 0.08, drift: 23.3%)\n",
      "  _c10: OK (baseline: 0.10, batch: 0.09, drift: 5.9%)\n",
      "  _c11: OK (baseline: 0.20, batch: 0.19, drift: 4.4%)\n",
      "  _c12: OK (baseline: 13.91, batch: 13.96, drift: 0.4%)\n",
      "  _c13: OK (baseline: 0.11, batch: 0.11, drift: 7.3%)\n",
      "  _c14: OK (baseline: 0.06, batch: 0.07, drift: 4.2%)\n",
      "\n",
      "Reports available at:\n",
      "s3://sagemaker-us-east-1-767397858887/sm-batch-monitoring/reports/data_quality_analysis/\n"
     ]
    }
   ],
   "source": [
    "# Simple drift detection\n",
    "print(\"\\nData Quality Summary:\")\n",
    "for baseline_feature in baseline_stats['features']:\n",
    "    feature_name = baseline_feature['name']\n",
    "    \n",
    "    # Find matching feature in batch stats\n",
    "    batch_feature = next((f for f in batch_stats['features'] if f['name'] == feature_name), None)\n",
    "    \n",
    "    if batch_feature:\n",
    "        # Get mean from numerical_statistics\n",
    "        baseline_mean = baseline_feature.get('numerical_statistics', {}).get('mean')\n",
    "        batch_mean = batch_feature.get('numerical_statistics', {}).get('mean')\n",
    "        \n",
    "        if baseline_mean is not None and batch_mean is not None:\n",
    "            drift_pct = abs(batch_mean - baseline_mean) / abs(baseline_mean) * 100 if baseline_mean != 0 else 0\n",
    "            status = \"DRIFT\" if drift_pct > 10 else \"OK\"\n",
    "            print(f\"  {feature_name}: {status} (baseline: {baseline_mean:.2f}, batch: {batch_mean:.2f}, drift: {drift_pct:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"  {feature_name}: Non-numerical feature\")\n",
    "\n",
    "print(f\"\\nReports available at:\\n{REPORTS}/data_quality_analysis/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmjVzMvzf18p"
   },
   "source": [
    "Model Quality Monitoring - Compare Predictions vs Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "Initialy tried using `ModelQualityMonitor` directly but it only works for real-time endpoints, not batch. So used`sklearn` instead (and formatted to match SageMaker Model Monitor output structure).\n",
    "Reports stored in S3 following SageMaker conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T08:28:27.978192Z",
     "iopub.status.busy": "2025-10-21T08:28:27.977897Z",
     "iopub.status.idle": "2025-10-21T08:28:28.518996Z",
     "shell.execute_reply": "2025-10-21T08:28:28.518190Z",
     "shell.execute_reply.started": "2025-10-21T08:28:27.978170Z"
    },
    "id": "ykNHMvK2LMtK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model Quality Metrics:\n",
      "  Accuracy:  0.4708\n",
      "  Precision: 0.4700\n",
      "  Recall:    0.4708\n",
      "  F1 Score:  0.4704\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1656  661  966]\n",
      " [ 627 1752 1012]\n",
      " [1003 1047 1321]]\n",
      "\n",
      "Model quality report:\n",
      "s3://sagemaker-us-east-1-767397858887/sm-batch-monitoring/reports/quality_analysis/model_quality_metrics.json\n"
     ]
    }
   ],
   "source": [
    "# Model Quality Monitoring: compare predicted labels (from softmax argmax) to ground truth.\n",
    "\n",
    "# Process predictions\n",
    "preds_uri = write_pred_labels_from_softprob()\n",
    "\n",
    "# Upload ground truth\n",
    "gt_uri    = upload_ground_truth_from_prod()\n",
    "\n",
    "# Download predictions & ground truth for analysis\n",
    "preds_bucket, preds_key = s3_split(preds_uri)\n",
    "gt_bucket, gt_key = s3_split(gt_uri)\n",
    "\n",
    "preds_body = s3.get_object(Bucket=preds_bucket, Key=preds_key)[\"Body\"].read()\n",
    "gt_body = s3.get_object(Bucket=gt_bucket, Key=gt_key)[\"Body\"].read()\n",
    "\n",
    "y_pred = pd.read_csv(io.BytesIO(preds_body), header=None)[0].values\n",
    "y_true = pd.read_csv(io.BytesIO(gt_body), header=None)[0].values\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Create model quality report\n",
    "model_quality_report = {\n",
    "    \"multiclass_classification_metrics\": {\n",
    "        \"accuracy\": {\"value\": float(accuracy)},\n",
    "        \"weighted_precision\": {\"value\": float(precision)},\n",
    "        \"weighted_recall\": {\"value\": float(recall)},\n",
    "        \"weighted_f1\": {\"value\": float(f1)}\n",
    "    },\n",
    "    \"confusion_matrix\": conf_matrix.tolist(),\n",
    "    \"classification_report\": classification_report(y_true, y_pred, output_dict=True)\n",
    "}\n",
    "\n",
    "# Save report to S3\n",
    "report_key = f\"{PREFIX}/reports/quality_analysis/model_quality_metrics.json\"\n",
    "s3.put_object(\n",
    "    Bucket=BUCKET,\n",
    "    Key=report_key,\n",
    "    Body=json.dumps(model_quality_report, indent=2).encode()\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n Model Quality Metrics:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")\n",
    "print(f\"\\n Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"\\nModel quality report:\\ns3://{BUCKET}/{report_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgkxtkGbgLS9"
   },
   "source": [
    "Create CloudWatch Dashboard for Visualization & Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T08:28:33.617456Z",
     "iopub.status.busy": "2025-10-21T08:28:33.617165Z",
     "iopub.status.idle": "2025-10-21T08:28:33.746632Z",
     "shell.execute_reply": "2025-10-21T08:28:33.745585Z",
     "shell.execute_reply.started": "2025-10-21T08:28:33.617431Z"
    },
    "id": "79lHdM2fgOny"
   },
   "outputs": [],
   "source": [
    "# Final Dashboard Creation\n",
    "create_dashboard()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
